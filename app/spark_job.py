import os
from pyspark.sql import SparkSession

def main() -> None:

    spark = (
            SparkSession.builder.appName("Appto_SnowFlake").getOrCreate()
    )

    # read postgres data
    jdbc_url = os.getenv("POSTGRES_URL")

    connection_properties = {
        "user": os.getenv("POSTGRES_USER"),
        "password": os.getenv("POSTGRES_PASSWORD"),
        "driver": "org.postgresql.Driver"
    }

    df = (
        spark.read.jdbc(
            url=jdbc_url,
            table="user",
            properties=connection_properties
        )
    )

    print("Data extracted from application")
    df.show()

    # Write data to Snowflake
    sfOptions = {
    "sfURL": f"{os.getenv("SNOWFLAKE_ACCOUNT")}.snowflakecomputing.com",
    "sfUser": os.getenv("SNOWFLAKE_USER"),
    "sfPassword": os.getenv("SNOWFLAKE_PASSWORD"),
    "sfDatabase": os.getenv("SNOWFLAKE_DATABASE"),
    "sfSchema": os.getenv("SNOWFLAKE_SCHEMA"),
     "sfWarehouse": os.getenv("SNOWFLAKE_WAREHOUSE") 
    }

    (
        df.write.format("snowflake")
        .options(**sfOptions)
        .option("dbtable", "user")
        .mode("overwrite")
        .save()
    )

    print("Data loaded to Snowflake successfully")

    spark.stop()


    return None 
if __name__ == "__main__":
    main()